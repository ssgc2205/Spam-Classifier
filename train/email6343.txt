For a relatively long time, I have known about The Wayback Machine and I always just assumed I can get a past version of any website whenever I want. That is, until I actually needed to. I haven't started any actual work yet, but I want to train a model for detecting nsfw and racy pictures. I started thinking about getting the data first. So I figured, I should just download every picture on [victoriassecret.com](https://victoriassecret.com) since the site has been created, each download separated by approximately a week, and remove the duplicates, checking by name. To my unpleasent surprise, when I tried downloading the entire website on a particular day (using wget), I barely got anything. The homepage is archived [many times a day](https://web.archive.org/web/*/www.victoriassecret.com), with a total 32,417 snapshots over time. But individual pages have been archived maybe once or twice since forever. And even those pages don't have anything. What I could understand is that I was falsely happy about being able to access any website at pretty much any point of time using The Wayback Machine. It has made me sad enough that I don't want to believe it. Can anyone confirm that what I have said is true, so that I give up on getting data this way.