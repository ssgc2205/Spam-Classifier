448 million search terms along with the last 24 month's worth of per-month search frequencies. Given the market share of the search engine from which this data came, multiplying these monthly counts by about 15 should yield the total search volume across all search engines.

Uncompressed, the file is about 53GB. Compressed it's about 5GB.

Each line of the file is a JSON object that looks like this:

    {"_id":{"$oid":"5d4f768d272acc57122804a3"},"keyphrase":"hackathon ideas","trend":[60,90,110,20,90,90,100,30,30,60,50,40,60,80,120,70,80,100,70,70,40,50,60,50]}

You can ignore the \_id property (I'd ideally have removed that property when I exported the data). The two relevant properties are "keyphrase" and "trend". The numbers at the end of the "trend" array are the most recent frequencies. Each item in the "trend" array is the number of searches that month, globally. The data was collected a few months before the end of 2019.

Note that some of the keyphrases have 4 dollar-signs in them like this:

    "keyphrase":"hackathon$$$$hackathon"

I'm not sure what this means (it's just what the API returned), so I just ignored these in my analyses. It may have something to do with new advanced search engine features like knowledge graphs, but I don't know.

[https://archive.org/details/2019-search-engine-keyphrases.json](https://archive.org/details/2019-search-engine-keyphrases.json)

[https://www.kaggle.com/hofesiy/2019-search-engine-keywords](https://www.kaggle.com/hofesiy/2019-search-engine-keywords)

Posting with a throwaway. Please mirror it somewhere if you can (and post link in comments) in case it gets taken down for whatever reason. Hope some people find this useful/interesting!

**Edit**: The final month in the trends array is August 2019.